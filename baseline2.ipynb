{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge list …\n",
      "Positive edges loaded: 54,138\n",
      "Splitting into train/val/test …\n",
      "Train edges: 37,896  |  Val edges: 8,121  |  Test edges: 8,121\n",
      "\n",
      "PA – Original test metrics:\n",
      "  AUC: 0.9517,  AP: 0.9550\n",
      "\n",
      "PA – Rare test metrics (≤25th‑percentile degree nodes):\n",
      "  AUC: 0.6336,  AP: 0.0080\n"
     ]
    }
   ],
   "source": [
    "# Preferential Attachment\n",
    "\n",
    "# 1. Load edge list (assumes columns: source,target,weight) \n",
    "EDGE_CSV = \"ingredient_cooccur_graph.csv\"  # path can be changed as needed\n",
    "\n",
    "print(\"Loading edge list …\")\n",
    "edges_df = pd.read_csv(EDGE_CSV, header=0)\n",
    "\n",
    "author_columns = [\"source\", \"target\"]\n",
    "if len(edges_df.columns) < 2:\n",
    "    raise ValueError(\"Edge CSV must contain at least two columns: source, target\")\n",
    "\n",
    "pos_edges = list(zip(edges_df[\"source\"], edges_df[\"target\"]))\n",
    "print(f\"Positive edges loaded: {len(pos_edges):,}\")\n",
    "\n",
    "\n",
    "# 2. Train / validation / test split \n",
    "print(\"Splitting into train/val/test …\")\n",
    "train_edges, tmp_edges = train_test_split(pos_edges, test_size=0.30, random_state=42)\n",
    "val_edges,  test_edges = train_test_split(tmp_edges, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Train edges: {len(train_edges):,}  |  Val edges: {len(val_edges):,}  |  Test edges: {len(test_edges):,}\")\n",
    "\n",
    "\n",
    "# 3. Build the training graph \n",
    "G_train = nx.Graph()\n",
    "all_nodes = set(edges_df[\"source\"]).union(set(edges_df[\"target\"]))\n",
    "G_train.add_nodes_from(all_nodes)\n",
    "G_train.add_edges_from(train_edges)\n",
    "\n",
    "degree_dict = dict(G_train.degree())  # Pre‑compute degrees for PA\n",
    "\n",
    "\n",
    "# 4. Preferential‑Attachment scoring function \n",
    "def pa_score(u: str, v: str) -> int:\n",
    "    \"\"\"PA score = degree(u) × degree(v).  If node unseen in train, degree=0.\"\"\"\n",
    "    return degree_dict.get(u, 0) * degree_dict.get(v, 0)\n",
    "\n",
    "\n",
    "# 5. Negative sampling helper \n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def sample_negative_edges(num_samples: int, forbidden: set) -> list:\n",
    "    \"\"\"Randomly sample `num_samples` non‑existing edges from the node set.\"\"\"\n",
    "    neg = set()\n",
    "    nodes = list(all_nodes)\n",
    "    while len(neg) < num_samples:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if (u, v) not in forbidden and (v, u) not in forbidden and not G_train.has_edge(u, v):\n",
    "            neg.add((u, v))\n",
    "    return list(neg)\n",
    "\n",
    "forbidden_pairs = set(pos_edges)\n",
    "neg_test_edges = sample_negative_edges(len(test_edges), forbidden_pairs)\n",
    "\n",
    "\n",
    "# 6. Evaluate on original test set\n",
    "test_pairs = test_edges + neg_test_edges\n",
    "test_labels = [1] * len(test_edges) + [0] * len(neg_test_edges)\n",
    "pa_scores  = [pa_score(u, v) for u, v in test_pairs]\n",
    "\n",
    "auc_orig = roc_auc_score(test_labels, pa_scores)\n",
    "ap_orig  = average_precision_score(test_labels, pa_scores)\n",
    "\n",
    "print(\"\\nPA – Original test metrics:\")\n",
    "print(f\"  AUC: {auc_orig:.4f},  AP: {ap_orig:.4f}\")\n",
    "\n",
    "\n",
    "# 7. Rare‑subset evaluation (nodes with degree ≤ 25th percentile) \n",
    "deg_values = list(degree_dict.values())\n",
    "if len(deg_values) == 0:\n",
    "    raise ValueError(\"Graph has no edges after train split – check data.\")\n",
    "\n",
    "deg_threshold = np.percentile(deg_values, 25)\n",
    "\n",
    "def is_rare(edge):\n",
    "    u, v = edge\n",
    "    return degree_dict.get(u, 0) <= deg_threshold and degree_dict.get(v, 0) <= deg_threshold\n",
    "\n",
    "rare_pos = [e for e in test_edges     if is_rare(e)]\n",
    "rare_neg = [e for e in neg_test_edges if is_rare(e)]\n",
    "\n",
    "if len(rare_pos) > 0 and len(rare_neg) > 0:\n",
    "    rare_pairs  = rare_pos + rare_neg\n",
    "    rare_labels = [1]*len(rare_pos) + [0]*len(rare_neg)\n",
    "    rare_scores = [pa_score(u, v) for u, v in rare_pairs]\n",
    "\n",
    "    auc_rare = roc_auc_score(rare_labels, rare_scores)\n",
    "    ap_rare  = average_precision_score(rare_labels, rare_scores)\n",
    "\n",
    "    print(\"\\nPA – Rare test metrics (≤25th‑percentile degree nodes):\")\n",
    "    print(f\"  AUC: {auc_rare:.4f},  AP: {ap_rare:.4f}\")\n",
    "else:\n",
    "    print(\"\\n[INFO] Not enough rare pairs to compute rare‑subset metrics.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge list …\n",
      "Total positive edges: 54,138\n",
      "Splitting into train/val/test …\n",
      "Train=37,896, Val=8,121, Test=8,121\n",
      "Generating random walks …\n",
      "Preparing skip-gram pairs …\n",
      "Total training pairs: 1,159,400\n",
      "Training Skip-gram …\n",
      "Epoch 01 | Skip-gram Loss: 3.2600\n",
      "Epoch 02 | Skip-gram Loss: 3.1947\n",
      "Epoch 03 | Skip-gram Loss: 3.1296\n",
      "Epoch 04 | Skip-gram Loss: 3.0649\n",
      "Epoch 05 | Skip-gram Loss: 3.0004\n",
      "Epoch 06 | Skip-gram Loss: 2.9360\n",
      "Epoch 07 | Skip-gram Loss: 2.8717\n",
      "Epoch 08 | Skip-gram Loss: 2.8073\n",
      "Epoch 09 | Skip-gram Loss: 2.7428\n",
      "Epoch 10 | Skip-gram Loss: 2.6780\n",
      "Epoch 11 | Skip-gram Loss: 2.6129\n",
      "Epoch 12 | Skip-gram Loss: 2.5473\n",
      "Epoch 13 | Skip-gram Loss: 2.4811\n",
      "Epoch 14 | Skip-gram Loss: 2.4144\n",
      "Epoch 15 | Skip-gram Loss: 2.3468\n",
      "Epoch 16 | Skip-gram Loss: 2.2785\n",
      "Epoch 17 | Skip-gram Loss: 2.2093\n",
      "Epoch 18 | Skip-gram Loss: 2.1392\n",
      "Epoch 19 | Skip-gram Loss: 2.0682\n",
      "Epoch 20 | Skip-gram Loss: 1.9963\n",
      "Epoch 21 | Skip-gram Loss: 1.9234\n",
      "Epoch 22 | Skip-gram Loss: 1.8497\n",
      "Epoch 23 | Skip-gram Loss: 1.7752\n",
      "Epoch 24 | Skip-gram Loss: 1.7001\n",
      "Epoch 25 | Skip-gram Loss: 1.6244\n",
      "Epoch 26 | Skip-gram Loss: 1.5483\n",
      "Epoch 27 | Skip-gram Loss: 1.4721\n",
      "Epoch 28 | Skip-gram Loss: 1.3959\n",
      "Epoch 29 | Skip-gram Loss: 1.3199\n",
      "Epoch 30 | Skip-gram Loss: 1.2445\n",
      "Epoch 31 | Skip-gram Loss: 1.1698\n",
      "Epoch 32 | Skip-gram Loss: 1.0963\n",
      "Epoch 33 | Skip-gram Loss: 1.0242\n",
      "Epoch 34 | Skip-gram Loss: 0.9537\n",
      "Epoch 35 | Skip-gram Loss: 0.8852\n",
      "Epoch 36 | Skip-gram Loss: 0.8189\n",
      "Epoch 37 | Skip-gram Loss: 0.7551\n",
      "Epoch 38 | Skip-gram Loss: 0.6940\n",
      "Epoch 39 | Skip-gram Loss: 0.6358\n",
      "Epoch 40 | Skip-gram Loss: 0.5807\n",
      "Epoch 41 | Skip-gram Loss: 0.5287\n",
      "Epoch 42 | Skip-gram Loss: 0.4800\n",
      "Epoch 43 | Skip-gram Loss: 0.4345\n",
      "Epoch 44 | Skip-gram Loss: 0.3923\n",
      "Epoch 45 | Skip-gram Loss: 0.3533\n",
      "Epoch 46 | Skip-gram Loss: 0.3174\n",
      "Epoch 47 | Skip-gram Loss: 0.2846\n",
      "Epoch 48 | Skip-gram Loss: 0.2547\n",
      "Epoch 49 | Skip-gram Loss: 0.2276\n",
      "Epoch 50 | Skip-gram Loss: 0.2031\n",
      "Epoch 51 | Skip-gram Loss: 0.1810\n",
      "Epoch 52 | Skip-gram Loss: 0.1612\n",
      "Epoch 53 | Skip-gram Loss: 0.1434\n",
      "Epoch 54 | Skip-gram Loss: 0.1276\n",
      "Epoch 55 | Skip-gram Loss: 0.1136\n",
      "Epoch 56 | Skip-gram Loss: 0.1011\n",
      "Epoch 57 | Skip-gram Loss: 0.0900\n",
      "Epoch 58 | Skip-gram Loss: 0.0802\n",
      "Epoch 59 | Skip-gram Loss: 0.0715\n",
      "Epoch 60 | Skip-gram Loss: 0.0639\n",
      "Epoch 61 | Skip-gram Loss: 0.0571\n",
      "Epoch 62 | Skip-gram Loss: 0.0511\n",
      "Epoch 63 | Skip-gram Loss: 0.0459\n",
      "Epoch 64 | Skip-gram Loss: 0.0412\n",
      "Epoch 65 | Skip-gram Loss: 0.0371\n",
      "Epoch 66 | Skip-gram Loss: 0.0334\n",
      "Epoch 67 | Skip-gram Loss: 0.0302\n",
      "Epoch 68 | Skip-gram Loss: 0.0273\n",
      "Epoch 69 | Skip-gram Loss: 0.0247\n",
      "Epoch 70 | Skip-gram Loss: 0.0225\n",
      "Epoch 71 | Skip-gram Loss: 0.0204\n",
      "Epoch 72 | Skip-gram Loss: 0.0186\n",
      "Epoch 73 | Skip-gram Loss: 0.0170\n",
      "Epoch 74 | Skip-gram Loss: 0.0156\n",
      "Epoch 75 | Skip-gram Loss: 0.0143\n",
      "Epoch 76 | Skip-gram Loss: 0.0132\n",
      "Epoch 77 | Skip-gram Loss: 0.0122\n",
      "Epoch 78 | Skip-gram Loss: 0.0112\n",
      "Epoch 79 | Skip-gram Loss: 0.0104\n",
      "Epoch 80 | Skip-gram Loss: 0.0096\n",
      "Epoch 81 | Skip-gram Loss: 0.0090\n",
      "Epoch 82 | Skip-gram Loss: 0.0083\n",
      "Epoch 83 | Skip-gram Loss: 0.0078\n",
      "Epoch 84 | Skip-gram Loss: 0.0073\n",
      "Epoch 85 | Skip-gram Loss: 0.0068\n",
      "Epoch 86 | Skip-gram Loss: 0.0064\n",
      "Epoch 87 | Skip-gram Loss: 0.0060\n",
      "Epoch 88 | Skip-gram Loss: 0.0056\n",
      "Epoch 89 | Skip-gram Loss: 0.0053\n",
      "Epoch 90 | Skip-gram Loss: 0.0050\n",
      "Epoch 91 | Skip-gram Loss: 0.0047\n",
      "Epoch 92 | Skip-gram Loss: 0.0045\n",
      "Epoch 93 | Skip-gram Loss: 0.0043\n",
      "Epoch 94 | Skip-gram Loss: 0.0040\n",
      "Epoch 95 | Skip-gram Loss: 0.0038\n",
      "Epoch 96 | Skip-gram Loss: 0.0037\n",
      "Epoch 97 | Skip-gram Loss: 0.0035\n",
      "Epoch 98 | Skip-gram Loss: 0.0033\n",
      "Epoch 99 | Skip-gram Loss: 0.0032\n",
      "Epoch 100 | Skip-gram Loss: 0.0031\n",
      "Sampling negatives …\n",
      "Training MLP …\n",
      "Epoch 05 | Train Loss: 0.6782 | Val Loss: 0.6755\n",
      "Epoch 10 | Train Loss: 0.6622 | Val Loss: 0.6601\n",
      "Epoch 15 | Train Loss: 0.6472 | Val Loss: 0.6458\n",
      "Epoch 20 | Train Loss: 0.6335 | Val Loss: 0.6331\n",
      "Epoch 25 | Train Loss: 0.6213 | Val Loss: 0.6220\n",
      "Epoch 30 | Train Loss: 0.6097 | Val Loss: 0.6112\n",
      "Epoch 35 | Train Loss: 0.5976 | Val Loss: 0.5998\n",
      "Epoch 40 | Train Loss: 0.5846 | Val Loss: 0.5872\n",
      "Epoch 45 | Train Loss: 0.5708 | Val Loss: 0.5737\n",
      "Epoch 50 | Train Loss: 0.5559 | Val Loss: 0.5591\n",
      "Epoch 55 | Train Loss: 0.5399 | Val Loss: 0.5435\n",
      "Epoch 60 | Train Loss: 0.5229 | Val Loss: 0.5270\n",
      "Epoch 65 | Train Loss: 0.5051 | Val Loss: 0.5097\n",
      "Epoch 70 | Train Loss: 0.4867 | Val Loss: 0.4918\n",
      "Epoch 75 | Train Loss: 0.4679 | Val Loss: 0.4736\n",
      "Epoch 80 | Train Loss: 0.4491 | Val Loss: 0.4554\n",
      "Epoch 85 | Train Loss: 0.4305 | Val Loss: 0.4375\n",
      "Epoch 90 | Train Loss: 0.4123 | Val Loss: 0.4202\n",
      "Epoch 95 | Train Loss: 0.3948 | Val Loss: 0.4036\n",
      "Epoch 100 | Train Loss: 0.3783 | Val Loss: 0.3879\n",
      "Evaluating …\n",
      "\n",
      "DeepWalk (torch) + MLP – Test Metrics:\n",
      "AUC: 0.9405, AP: 0.9391\n"
     ]
    }
   ],
   "source": [
    "#  DeepWalk (torch) + MLP \n",
    "#  1. Perform random walks on graph.\n",
    "#  2. Train Skip-gram model using PyTorch.\n",
    "#  3. Train MLP for link prediction.\n",
    "#  No gensim needed.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------- Hyper-parameters -------------------------------\n",
    "EDGE_CSV = \"ingredient_cooccur_graph.csv\"\n",
    "EMB_DIM  = 64\n",
    "WALK_LEN = 20\n",
    "NUM_WALKS = 10\n",
    "WINDOW_SIZE = 5\n",
    "SG_EPOCHS = 100\n",
    "MLP_EPOCHS = 100\n",
    "LR_SG = 0.01\n",
    "LR_MLP = 0.001\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------- 1. Load edge list ------------------------------\n",
    "print(\"Loading edge list …\")\n",
    "edf = pd.read_csv(EDGE_CSV, header=0)\n",
    "positives = list(zip(edf[\"source\"], edf[\"target\"]))\n",
    "all_nodes = set(edf[\"source\"]).union(edf[\"target\"])\n",
    "print(f\"Total positive edges: {len(positives):,}\")\n",
    "\n",
    "# ---------------------------- 2. Split data ----------------------------------\n",
    "print(\"Splitting into train/val/test …\")\n",
    "train_pos, tmp_pos = train_test_split(positives, test_size=0.30, random_state=SEED)\n",
    "val_pos,   test_pos = train_test_split(tmp_pos,    test_size=0.50, random_state=SEED)\n",
    "print(f\"Train={len(train_pos):,}, Val={len(val_pos):,}, Test={len(test_pos):,}\")\n",
    "\n",
    "# ---------------------------- 3. Build graph ---------------------------------\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(all_nodes)\n",
    "G.add_edges_from(train_pos)\n",
    "nodes = list(G.nodes())\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------- 4. Random Walks -------------------------------\n",
    "print(\"Generating random walks …\")\n",
    "def generate_walks(G, num_walks, walk_length):\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = [node]\n",
    "            while len(walk) < walk_length:\n",
    "                neighbors = list(G.neighbors(walk[-1]))\n",
    "                if neighbors:\n",
    "                    walk.append(random.choice(neighbors))\n",
    "                else:\n",
    "                    break\n",
    "            walks.append(walk)\n",
    "    return walks\n",
    "\n",
    "walks = generate_walks(G, NUM_WALKS, WALK_LEN)\n",
    "\n",
    "# ---------------------------- 5. Prepare Skip-gram Training Data ------------\n",
    "print(\"Preparing skip-gram pairs …\")\n",
    "sg_pairs = []\n",
    "for walk in walks:\n",
    "    for i, center in enumerate(walk):\n",
    "        window = walk[max(0, i-WINDOW_SIZE):i] + walk[i+1:i+1+WINDOW_SIZE]\n",
    "        for context in window:\n",
    "            sg_pairs.append((node_to_idx[center], node_to_idx[context]))\n",
    "\n",
    "print(f\"Total training pairs: {len(sg_pairs):,}\")\n",
    "\n",
    "# ---------------------------- 6. Define Skip-gram Model ----------------------\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, num_nodes, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_nodes, emb_dim)\n",
    "\n",
    "    def forward(self, center, context):\n",
    "        center_emb = self.emb(center)\n",
    "        context_emb = self.emb(context)\n",
    "        scores = (center_emb * context_emb).sum(dim=1)\n",
    "        return scores\n",
    "\n",
    "num_nodes = len(nodes)\n",
    "sg_model = SkipGram(num_nodes, EMB_DIM)\n",
    "optimizer_sg = torch.optim.Adam(sg_model.parameters(), lr=LR_SG)\n",
    "loss_fn_sg = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---------------------------- 7. Train Skip-gram -----------------------------\n",
    "print(\"Training Skip-gram …\")\n",
    "centers = torch.tensor([c for c, _ in sg_pairs], dtype=torch.long)\n",
    "contexts = torch.tensor([c for _, c in sg_pairs], dtype=torch.long)\n",
    "labels = torch.ones(len(sg_pairs))\n",
    "\n",
    "for epoch in range(1, SG_EPOCHS+1):\n",
    "    sg_model.train()\n",
    "    optimizer_sg.zero_grad()\n",
    "    preds = sg_model(centers, contexts)\n",
    "    loss = loss_fn_sg(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer_sg.step()\n",
    "    print(f\"Epoch {epoch:02d} | Skip-gram Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Get final embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = sg_model.emb.weight.data.cpu().numpy()\n",
    "\n",
    "# ---------------------------- 8. Edge Feature Construction ------------------\n",
    "def get_edge_feature(u, v):\n",
    "    \"\"\"Concatenate the embeddings for edge (u,v) into a feature vector.\"\"\"\n",
    "    if u not in node_to_idx or v not in node_to_idx:\n",
    "        return np.zeros(2 * EMB_DIM)\n",
    "    emb_u = embeddings[node_to_idx[u]]\n",
    "    emb_v = embeddings[node_to_idx[v]]\n",
    "    return np.concatenate([emb_u, emb_v])\n",
    "\n",
    "# ---------------------------- 9. Negative Sampling ---------------------------\n",
    "print(\"Sampling negatives …\")\n",
    "def sample_neg(G, k, forb):\n",
    "    neg = set()\n",
    "    while len(neg) < k:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if (u, v) not in forb and (v, u) not in forb and not G.has_edge(u, v):\n",
    "            neg.add((u, v))\n",
    "    return list(neg)\n",
    "\n",
    "forbidden = set(positives)\n",
    "neg_train = sample_neg(G, len(train_pos), forbidden)\n",
    "neg_val   = sample_neg(G, len(val_pos), forbidden)\n",
    "neg_test  = sample_neg(G, len(test_pos), forbidden)\n",
    "\n",
    "# ---------------------------- 10. Build datasets -----------------------------\n",
    "X_train = np.array([get_edge_feature(u,v) for u,v in train_pos+neg_train])\n",
    "y_train = np.array([1]*len(train_pos) + [0]*len(neg_train))\n",
    "\n",
    "X_val = np.array([get_edge_feature(u,v) for u,v in val_pos+neg_val])\n",
    "y_val = np.array([1]*len(val_pos) + [0]*len(neg_val))\n",
    "\n",
    "X_test = np.array([get_edge_feature(u,v) for u,v in test_pos+neg_test])\n",
    "y_test = np.array([1]*len(test_pos) + [0]*len(neg_test))\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------- 11. Define and Train MLP -----------------------\n",
    "class EdgeMLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "print(\"Training MLP …\")\n",
    "model = EdgeMLP(2*EMB_DIM)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR_MLP)\n",
    "\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(1, MLP_EPOCHS+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_train)\n",
    "    loss = loss_fn(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val)\n",
    "            val_loss = loss_fn(val_logits, y_val)\n",
    "            print(f\"Epoch {epoch:02d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# ---------------------------- 12. Evaluate -----------------------------------\n",
    "print(\"Evaluating …\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = torch.sigmoid(model(X_test)).cpu().numpy()\n",
    "    auc = roc_auc_score(y_test.cpu().numpy(), pred)\n",
    "    ap  = average_precision_score(y_test.cpu().numpy(), pred)\n",
    "\n",
    "print(f\"\\nDeepWalk (torch) + MLP – Test Metrics:\")\n",
    "print(f\"AUC: {auc:.4f}, AP: {ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge list …\n",
      "Total edges: 54,138\n",
      "Splitting into train/val/test …\n",
      "Train=37,896, Val=8,121, Test=8,121\n",
      "Building NetworkX training graph …\n",
      "Starting LINE training …\n",
      "Epoch 1/20 – Loss: 2.3991\n",
      "Epoch 2/20 – Loss: 2.6548\n",
      "Epoch 3/20 – Loss: 2.6645\n",
      "Epoch 4/20 – Loss: 2.7025\n",
      "Epoch 5/20 – Loss: 2.7545\n",
      "Epoch 6/20 – Loss: 2.7518\n",
      "Epoch 7/20 – Loss: 2.7307\n",
      "Epoch 8/20 – Loss: 2.7718\n",
      "Epoch 9/20 – Loss: 2.7897\n",
      "Epoch 10/20 – Loss: 2.7998\n",
      "Epoch 11/20 – Loss: 2.7890\n",
      "Epoch 12/20 – Loss: 2.7767\n",
      "Epoch 13/20 – Loss: 2.8002\n",
      "Epoch 14/20 – Loss: 2.8095\n",
      "Epoch 15/20 – Loss: 2.8297\n",
      "Epoch 16/20 – Loss: 2.8154\n",
      "Epoch 17/20 – Loss: 2.8080\n",
      "Epoch 18/20 – Loss: 2.7988\n",
      "Epoch 19/20 – Loss: 2.8114\n",
      "Epoch 20/20 – Loss: 2.8241\n",
      "Evaluating LINE embeddings …\n",
      "LINE – Test AUC: 0.4082, AP: 0.4184\n",
      "LINE – Rare AUC: 0.5488, AP: 0.0058\n"
     ]
    }
   ],
   "source": [
    "# LINE(Large-scale Information Network Embedding): first-order\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------------------- Hyper-parameters -------------------------------\n",
    "EDGE_CSV       = \"ingredient_cooccur_graph.csv\"  # path to CSV with columns: source,target,weight\n",
    "EMB_DIM        = 64\n",
    "EPOCHS         = 20\n",
    "LR             = 0.01\n",
    "NEG_SAMPLES    = 5   # number of negative samples per positive edge\n",
    "SEED           = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------------------- 1. Load edge list ------------------------------\n",
    "print(\"Loading edge list …\")\n",
    "edf = pd.read_csv(EDGE_CSV)\n",
    "# if weight column not present, assume weight=1\n",
    "if \"weight\" not in edf.columns:\n",
    "    edf[\"weight\"] = 1.0\n",
    "edges = list(zip(edf[\"source\"], edf[\"target\"], edf[\"weight\"]))\n",
    "all_nodes = sorted(set(edf[\"source\"]).union(edf[\"target\"]))\n",
    "print(f\"Total edges: {len(edges):,}\")\n",
    "\n",
    "# ---------------------------- 2. Split data ----------------------------------\n",
    "print(\"Splitting into train/val/test …\")\n",
    "pairs = [(u, v) for u, v, _ in edges]\n",
    "train_pos, tmp = train_test_split(pairs, test_size=0.30, random_state=SEED)\n",
    "pos_val, test_pos = train_test_split(tmp, test_size=0.50, random_state=SEED)\n",
    "print(f\"Train={len(train_pos):,}, Val={len(pos_val):,}, Test={len(test_pos):,}\")\n",
    "\n",
    "# ---------------------------- 3. Build training graph ------------------------\n",
    "print(\"Building NetworkX training graph …\")\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(all_nodes)\n",
    "G.add_weighted_edges_from([(u, v, w) for u, v, w in edges if (u, v) in train_pos])\n",
    "\n",
    "node_to_idx = {n: i for i, n in enumerate(all_nodes)}\n",
    "num_nodes = len(all_nodes)\n",
    "\n",
    "# ---------------------------- 4. Define LINE model ---------------------------\n",
    "class LINEFirstOrder(nn.Module):\n",
    "    def __init__(self, num_nodes, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_u = nn.Embedding(num_nodes, emb_dim)\n",
    "        self.emb_v = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.emb_u.weight)\n",
    "        nn.init.xavier_uniform_(self.emb_v.weight)\n",
    "\n",
    "    def forward(self, u_idx, v_idx, neg_v_idx):\n",
    "        # positive score\n",
    "        u_emb = self.emb_u(u_idx)\n",
    "        v_emb = self.emb_v(v_idx)\n",
    "        pos_score = torch.sum(u_emb * v_emb, dim=-1)\n",
    "        # negative scores: broadcast\n",
    "        neg_emb = self.emb_v(neg_v_idx)  # shape: [B, NEG, D]\n",
    "        u_emb_neg = u_emb.unsqueeze(1)   # shape: [B, 1, D]\n",
    "        neg_score = torch.bmm(neg_emb, u_emb_neg.transpose(1,2)).squeeze(-1)\n",
    "        return pos_score, neg_score\n",
    "\n",
    "model = LINEFirstOrder(num_nodes, EMB_DIM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---------------------------- 5. Negative Sampling ---------------------------\n",
    "nodes = all_nodes\n",
    "all_idx = list(range(num_nodes))\n",
    "\n",
    "print(\"Starting LINE training …\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    random.shuffle(train_pos)\n",
    "    for u, v in train_pos:\n",
    "        u_idx = torch.tensor([node_to_idx[u]], dtype=torch.long)\n",
    "        v_idx = torch.tensor([node_to_idx[v]], dtype=torch.long)\n",
    "        # sample negatives\n",
    "        neg_vs = random.choices(all_idx, k=NEG_SAMPLES)\n",
    "        neg_v_idx = torch.tensor([neg_vs], dtype=torch.long)\n",
    "        # forward\n",
    "        pos_score, neg_score = model(u_idx, v_idx, neg_v_idx)\n",
    "        # labels\n",
    "        pos_label = torch.ones_like(pos_score)\n",
    "        neg_label = torch.zeros_like(neg_score)\n",
    "        # compute loss\n",
    "        loss = loss_fn(pos_score, pos_label)\n",
    "        loss += loss_fn(neg_score, neg_label).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} – Loss: {total_loss/len(train_pos):.4f}\")\n",
    "\n",
    "# ---------------------------- 6. Extract embeddings -------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    emb_u = model.emb_u.weight.data.cpu().numpy()\n",
    "    emb_v = model.emb_v.weight.data.cpu().numpy()\n",
    "# for first-order, you can average emb_u and emb_v or just use emb_u\n",
    "emb = (emb_u + emb_v) / 2.0  # final node embedding matrix\n",
    "\n",
    "# ---------------------------- 7. Evaluate link prediction -------------------\n",
    "print(\"Evaluating LINE embeddings …\")\n",
    "def edge_score(u, v):\n",
    "    ui, vi = node_to_idx[u], node_to_idx[v]\n",
    "    return float(np.dot(emb[ui], emb[vi]))\n",
    "\n",
    "# prepare test negatives\n",
    "forbidden = set(pairs)\n",
    "neg_test = []\n",
    "while len(neg_test) < len(test_pos):\n",
    "    u, v = random.sample(all_nodes, 2)\n",
    "    if (u, v) not in forbidden and (v, u) not in forbidden and not G.has_edge(u, v):\n",
    "        neg_test.append((u, v))\n",
    "\n",
    "test_pairs = test_pos + neg_test\n",
    "test_labels = [1]*len(test_pos) + [0]*len(neg_test)\n",
    "\n",
    "scores = [edge_score(u, v) for u, v in test_pairs]\n",
    "auc = roc_auc_score(test_labels, scores)\n",
    "ap = average_precision_score(test_labels, scores)\n",
    "print(f\"LINE – Test AUC: {auc:.4f}, AP: {ap:.4f}\")\n",
    "\n",
    "# ---------------------------- 8. Rare subset --------------------------------\n",
    "deg = dict(G.degree())\n",
    "thresh = np.percentile(list(deg.values()), 25)\n",
    "rare_pos = [e for e in test_pos if deg[e[0]]<=thresh and deg[e[1]]<=thresh]\n",
    "rare_neg = [e for e in neg_test if deg[e[0]]<=thresh and deg[e[1]]<=thresh]\n",
    "\n",
    "if rare_pos and rare_neg:\n",
    "    rare_pairs = rare_pos + rare_neg\n",
    "    rare_labels = [1]*len(rare_pos) + [0]*len(rare_neg)\n",
    "    rare_scores = [edge_score(u, v) for u, v in rare_pairs]\n",
    "    auc_r = roc_auc_score(rare_labels, rare_scores)\n",
    "    ap_r  = average_precision_score(rare_labels, rare_scores)\n",
    "    print(f\"LINE – Rare AUC: {auc_r:.4f}, AP: {ap_r:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough rare-node test pairs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge list …\n",
      "Nodes: 686, Edges: 54138\n",
      "Train pos: 37896, Val pos: 8121, Test pos: 8121\n",
      "Building adjacency matrix …\n",
      "Computing Katz index matrix (S) …\n",
      "Performing Truncated SVD for 64 dims …\n",
      "Generating negative test edges …\n",
      "Evaluating on original test set …\n",
      "HOPE – Original AUC: 0.4760, AP: 0.4637\n",
      "Evaluating on rare-node subset …\n",
      "HOPE – Rare AUC: 0.8460, AP: 0.2547\n"
     ]
    }
   ],
   "source": [
    "# HOPE (High-Order Proximity preserved Embedding)\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# ---------------------------- Hyper-parameters -------------------------------\n",
    "EDGE_CSV = \"ingredient_cooccur_graph.csv\"  # must contain source,target,weight\n",
    "EMB_DIM = 64\n",
    "TEST_RATIO = 0.3\n",
    "VAL_RATIO = 0.5\n",
    "BETA = 0.01    # decay factor for Katz\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------------------------- 1. Load data -----------------------------------\n",
    "print(\"Loading edge list …\")\n",
    "edf = pd.read_csv(EDGE_CSV)\n",
    "if \"weight\" not in edf.columns:\n",
    "    edf[\"weight\"] = 1.0\n",
    "edges = list(zip(edf[\"source\"], edf[\"target\"], edf[\"weight\"]))\n",
    "nodes = sorted(set(edf[\"source\"]).union(edf[\"target\"]))\n",
    "node_to_idx = {n: i for i, n in enumerate(nodes)}\n",
    "n = len(nodes)\n",
    "print(f\"Nodes: {n}, Edges: {len(edges)}\")\n",
    "\n",
    "# ---------------------------- 2. Train/Val/Test split ------------------------\n",
    "pairs = [(u, v) for u, v, _ in edges]\n",
    "train_pos, tmp = train_test_split(pairs, test_size=TEST_RATIO, random_state=SEED)\n",
    "val_pos, test_pos = train_test_split(tmp, test_size=VAL_RATIO, random_state=SEED)\n",
    "print(f\"Train pos: {len(train_pos)}, Val pos: {len(val_pos)}, Test pos: {len(test_pos)}\")\n",
    "\n",
    "# ---------------------------- 3. Build adjacency -----------------------------\n",
    "print(\"Building adjacency matrix …\")\n",
    "A = np.zeros((n, n))\n",
    "for u, v, w in edges:\n",
    "    if (u, v) in train_pos or (v, u) in train_pos:\n",
    "        i, j = node_to_idx[u], node_to_idx[v]\n",
    "        A[i, j] = w\n",
    "        A[j, i] = w\n",
    "\n",
    "# ---------------------------- 4. Compute Katz proximity -----------------------\n",
    "print(\"Computing Katz index matrix (S) …\")\n",
    "I = np.eye(n)\n",
    "# S = (I - beta*A)^{-1} - I\n",
    "M = I - BETA * A\n",
    "# use pseudo-inverse if singular\n",
    "try:\n",
    "    Minv = np.linalg.inv(M)\n",
    "except np.linalg.LinAlgError:\n",
    "    Minv = np.linalg.pinv(M)\n",
    "S = Minv - I\n",
    "# replace NaNs and infinite values with zero\n",
    "S = np.nan_to_num(S, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ---------------------------- 5. Truncated SVD -------------------------------- Truncated SVD --------------------------------\n",
    "print(f\"Performing Truncated SVD for {EMB_DIM} dims …\")\n",
    "svd = TruncatedSVD(n_components=EMB_DIM, random_state=SEED)\n",
    "embeddings = svd.fit_transform(S)\n",
    "# normalize embeddings with safe division\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "# avoid division by zero\n",
    "norms[norms == 0] = 1.0\n",
    "embeddings = embeddings / norms\n",
    "# ensure no NaNs\n",
    "embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ---------------------------- Helper: edge score -----------------------------\n",
    "def edge_score(u, v):\n",
    "    i, j = node_to_idx[u], node_to_idx[v]\n",
    "    return float(np.dot(embeddings[i], embeddings[j]))\n",
    "\n",
    "# ---------------------------- 6. Negative sampling ---------------------------\n",
    "print(\"Generating negative test edges …\")\n",
    "forbidden = set(pairs)\n",
    "neg_test = set()\n",
    "all_nodes = list(nodes)\n",
    "while len(neg_test) < len(test_pos):\n",
    "    u, v = random.sample(all_nodes, 2)\n",
    "    if (u, v) not in forbidden and (v, u) not in forbidden:\n",
    "        neg_test.add((u, v))\n",
    "neg_test = list(neg_test)\n",
    "\n",
    "# ---------------------------- 7. Evaluate on original ------------------------\n",
    "print(\"Evaluating on original test set …\")\n",
    "test_pairs = test_pos + neg_test\n",
    "test_labels = [1]*len(test_pos) + [0]*len(neg_test)\n",
    "scores = [edge_score(u, v) for u, v in test_pairs]\n",
    "auc = roc_auc_score(test_labels, scores)\n",
    "ap = average_precision_score(test_labels, scores)\n",
    "print(f\"HOPE – Original AUC: {auc:.4f}, AP: {ap:.4f}\")\n",
    "\n",
    "# ---------------------------- 8. Rare subset ----------------------------------\n",
    "print(\"Evaluating on rare-node subset …\")\n",
    "G_train = nx.Graph()\n",
    "G_train.add_nodes_from(nodes)\n",
    "G_train.add_edges_from(train_pos)\n",
    "deg = dict(G_train.degree())\n",
    "thresh = np.percentile(list(deg.values()), 25)\n",
    "rare_pos = [e for e in test_pos if deg[e[0]]<=thresh and deg[e[1]]<=thresh]\n",
    "rare_neg = [e for e in neg_test if deg[e[0]]<=thresh and deg[e[1]]<=thresh]\n",
    "if rare_pos and rare_neg:\n",
    "    pairs_r = rare_pos + rare_neg\n",
    "    labels_r = [1]*len(rare_pos) + [0]*len(rare_neg)\n",
    "    scores_r = [edge_score(u, v) for u, v in pairs_r]\n",
    "    auc_r = roc_auc_score(labels_r, scores_r)\n",
    "    ap_r = average_precision_score(labels_r, scores_r)\n",
    "    print(f\"HOPE – Rare AUC: {auc_r:.4f}, AP: {ap_r:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough rare-node pairs for evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
