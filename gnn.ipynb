{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxprtd2yhUj9",
        "outputId": "5a8b5b76-1f4c-423f-e5c8-99cb3e75c1fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original test metrics:\n",
            "  Cooc → AUC: 0.5000, AP: 0.5000\n",
            "  PMI  → AUC: 0.5000, AP: 0.5000\n",
            "  CN   → AUC: 0.9335, AP: 0.9387\n",
            "  AA   → AUC: 0.9347, AP: 0.9409\n",
            "  SVD  → AUC: 0.7501, AP: 0.7180\n",
            "\n",
            "Rare test metrics:\n",
            "  Cooc → AUC: 0.5000, AP: 0.0029\n",
            "  PMI  → AUC: 0.5000, AP: 0.0029\n",
            "  CN   → AUC: 0.9895, AP: 0.1422\n",
            "  AA   → AUC: 0.9942, AP: 0.2667\n",
            "  SVD  → AUC: 0.9769, AP: 0.5294\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# 1. 读取边列表，并保留权重\n",
        "df_edges = pd.read_csv(\"ingredient_cooccur_graph.csv\")\n",
        "edges = list(zip(df_edges[\"source\"], df_edges[\"target\"]))\n",
        "weights = {\n",
        "    (u, v): w\n",
        "    for u, v, w in zip(df_edges[\"source\"], df_edges[\"target\"], df_edges[\"weight\"])\n",
        "}\n",
        "\n",
        "# 2. 拆分正例\n",
        "e_train, e_tmp = train_test_split(edges, test_size=0.30, random_state=42)\n",
        "e_val, e_test = train_test_split(e_tmp, test_size=0.50, random_state=42)\n",
        "\n",
        "# 3. 构建训练图（加权）\n",
        "G_train = nx.Graph()\n",
        "G_train.add_nodes_from(set(df_edges[\"source\"]) | set(df_edges[\"target\"]))\n",
        "G_train.add_weighted_edges_from([\n",
        "    (u, v, weights.get((u, v), weights.get((v, u), 1)))\n",
        "    for u, v in e_train\n",
        "])\n",
        "nodes = list(G_train.nodes())\n",
        "\n",
        "# 4. 计算 cooc 和 PMI 所需的统计量\n",
        "train_cooc = nx.get_edge_attributes(G_train, 'weight')\n",
        "total_pairs = sum(train_cooc.values())\n",
        "marginal = {}\n",
        "for (u, v), w in train_cooc.items():\n",
        "    marginal[u] = marginal.get(u, 0) + w\n",
        "    marginal[v] = marginal.get(v, 0) + w\n",
        "pmi = {}\n",
        "for (u, v), w in train_cooc.items():\n",
        "    pi, pj = marginal[u] / total_pairs, marginal[v] / total_pairs\n",
        "    pij = w / total_pairs\n",
        "    pmi[(u, v)] = np.log(pij / (pi * pj) + 1e-9)\n",
        "\n",
        "# 5. 构建矩阵 A 并做 Truncated SVD 嵌入\n",
        "idx_map = { node: i for i, node in enumerate(nodes) }\n",
        "A = np.zeros((len(nodes), len(nodes)))\n",
        "for (u, v), w in train_cooc.items():\n",
        "    i, j = idx_map[u], idx_map[v]\n",
        "    A[i, j] = w\n",
        "    A[j, i] = w\n",
        "svd = TruncatedSVD(n_components=20, random_state=42)\n",
        "emb = normalize(svd.fit_transform(A))\n",
        "\n",
        "def svd_cos(u, v):\n",
        "    return float(np.dot(emb[idx_map[u]], emb[idx_map[v]]))\n",
        "\n",
        "# 6. 定义 CN 和 AA\n",
        "cn  = lambda u, v: len(list(nx.common_neighbors(G_train, u, v)))\n",
        "aa_index = { (u, v): s for u, v, s in nx.adamic_adar_index(G_train) }\n",
        "\n",
        "# 7. 负采样（测试集）\n",
        "def sample_neg_edges(G, n, forbidden):\n",
        "    negs = set()\n",
        "    while len(negs) < n:\n",
        "        u, v = np.random.choice(nodes, 2, replace=False)\n",
        "        if (u, v) not in forbidden and (v, u) not in forbidden and not G.has_edge(u, v):\n",
        "            negs.add((u, v))\n",
        "    return list(negs)\n",
        "\n",
        "forbidden = set(edges)\n",
        "e_test_neg = sample_neg_edges(G_train, len(e_test), forbidden)\n",
        "\n",
        "# 8. 在原始测试集上评估五种方法\n",
        "test_pairs_orig  = e_test + e_test_neg\n",
        "test_labels_orig = [1]*len(e_test) + [0]*len(e_test_neg)\n",
        "\n",
        "scores_orig = {\n",
        "    'Cooc': [train_cooc.get(p, train_cooc.get((p[1],p[0]), 0)) for p in test_pairs_orig],\n",
        "    'PMI':  [pmi.get(p,  pmi.get((p[1],p[0]), 0))               for p in test_pairs_orig],\n",
        "    'CN':   [cn(u, v)                                          for u, v in test_pairs_orig],\n",
        "    'AA':   [aa_index.get(p, aa_index.get((p[1],p[0]), 0))    for p in test_pairs_orig],\n",
        "    'SVD':  [svd_cos(u, v)                                     for u, v in test_pairs_orig],\n",
        "}\n",
        "\n",
        "print(\"Original test metrics:\")\n",
        "for name, sc in scores_orig.items():\n",
        "    auc = roc_auc_score(test_labels_orig, sc)\n",
        "    ap  = average_precision_score(test_labels_orig, sc)\n",
        "    print(f\"  {name:4s} → AUC: {auc:.4f}, AP: {ap:.4f}\")\n",
        "\n",
        "# 9. Rare 子集测试（度在前 25% 的节点）\n",
        "deg = dict(G_train.degree())\n",
        "threshold = np.percentile(list(deg.values()), 25)\n",
        "rare_pos = [e for e in e_test     if deg[e[0]] <= threshold and deg[e[1]] <= threshold]\n",
        "rare_neg = [e for e in e_test_neg if deg[e[0]] <= threshold and deg[e[1]] <= threshold]\n",
        "\n",
        "test_pairs_rare  = rare_pos + rare_neg\n",
        "test_labels_rare = [1]*len(rare_pos) + [0]*len(rare_neg)\n",
        "\n",
        "scores_rare = {\n",
        "    'Cooc': [train_cooc.get(p, train_cooc.get((p[1],p[0]), 0)) for p in test_pairs_rare],\n",
        "    'PMI':  [pmi.get(p,  pmi.get((p[1],p[0]), 0))               for p in test_pairs_rare],\n",
        "    'CN':   [cn(u, v)                                          for u, v in test_pairs_rare],\n",
        "    'AA':   [aa_index.get(p, aa_index.get((p[1],p[0]), 0))    for p in test_pairs_rare],\n",
        "    'SVD':  [svd_cos(u, v)                                     for u, v in test_pairs_rare],\n",
        "}\n",
        "\n",
        "print(\"\\nRare test metrics:\")\n",
        "for name, sc in scores_rare.items():\n",
        "    auc = roc_auc_score(test_labels_rare, sc)\n",
        "    ap  = average_precision_score(test_labels_rare, sc)\n",
        "    print(f\"  {name:4s} → AUC: {auc:.4f}, AP: {ap:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCN + Cos"
      ],
      "metadata": {
        "id": "2zWQRxvtklVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# — 1. Load edge list\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "df_edges = pd.read_csv(\"ingredient_cooccur_graph.csv\")\n",
        "edges     = list(zip(df_edges[\"source\"], df_edges[\"target\"]))\n",
        "weights   = {(u, v): w for u, v, w in zip(df_edges[\"source\"], df_edges[\"target\"], df_edges[\"weight\"]) }\n",
        "\n",
        "# — 2. Load features\n",
        "df_feat    = pd.read_csv(\"feature_matrix.csv\", index_col=\"node\")\n",
        "\n",
        "# —— 保持和训练图完全一致的节点顺序\n",
        "nodes_full = sorted(set(df_edges[\"source\"]) | set(df_edges[\"target\"]))\n",
        "df_feat    = df_feat.loc[nodes_full]\n",
        "\n",
        "# —— 构造张量\n",
        "X          = torch.tensor(df_feat.values.astype(np.float32), device=device)\n",
        "\n",
        "# —— 建立索引映射\n",
        "idx_map    = {n: i for i, n in enumerate(nodes_full)}\n",
        "\n",
        "\n",
        "# — 3. Split edges\n",
        "e_train, e_tmp  = train_test_split(edges, test_size=0.30, random_state=42)\n",
        "e_val,   e_test = train_test_split(e_tmp,   test_size=0.50, random_state=42)\n",
        "\n",
        "# — 4. Build train graph\n",
        "G_train = nx.Graph()\n",
        "G_train.add_nodes_from(nodes_full)\n",
        "G_train.add_weighted_edges_from([\n",
        "    (u, v, weights.get((u, v), weights.get((v, u), 1.0)))\n",
        "    for u, v in e_train\n",
        "])\n",
        "\n",
        "# compute degree for rare-split (you can use weighted degree if preferred)\n",
        "deg = dict(G_train.degree())\n",
        "\n",
        "# — 5. Negative sampling helper\n",
        "def sample_neg(n, forbidden, G):\n",
        "    negs = set()\n",
        "    while len(negs) < n:\n",
        "        u, v = np.random.choice(nodes_full, 2, replace=False)\n",
        "        if ((u, v) not in forbidden and (v, u) not in forbidden and not G.has_edge(u, v)):\n",
        "            negs.add((u, v))\n",
        "    return list(negs)\n",
        "\n",
        "forbidden   = set(edges)\n",
        "e_train_neg = sample_neg(len(e_train), forbidden, G_train)\n",
        "e_val_neg   = sample_neg(len(e_val),   forbidden, G_train)\n",
        "e_test_neg  = sample_neg(len(e_test),  forbidden, G_train)\n",
        "\n",
        "# — 6. Build normalized adjacency\n",
        "N = len(nodes_full)\n",
        "A = np.zeros((N, N), dtype=np.float32)\n",
        "for u, v in e_train:\n",
        "    i, j = idx_map[u], idx_map[v]\n",
        "    w    = weights.get((u, v), weights.get((v, u), 1.0))\n",
        "    A[i, j] = A[j, i] = w\n",
        "\n",
        "deg_array = A.sum(axis=1)\n",
        "D_inv_s   = np.diag(1.0 / np.sqrt(deg_array + 1e-9))\n",
        "A_norm    = torch.tensor(D_inv_s @ A @ D_inv_s, device=device)\n",
        "\n",
        "# — 7. Define GCN\n",
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim=64):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(in_dim, hid_dim)\n",
        "        self.lin2 = nn.Linear(hid_dim, hid_dim)\n",
        "    def encode(self, x, A):\n",
        "        h = A @ x\n",
        "        h = F.relu(self.lin1(h))\n",
        "        h = A @ h\n",
        "        h = F.relu(self.lin2(h))\n",
        "        return h\n",
        "\n",
        "# Instantiate\n",
        "model     = SimpleGCN(in_dim=X.shape[1], hid_dim=64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Prepare data\n",
        "train_pairs = e_train + e_train_neg\n",
        "train_lbls  = torch.cat([torch.ones(len(e_train)), torch.zeros(len(e_train_neg))], dim=0).to(device)\n",
        "val_pairs   = e_val   + e_val_neg\n",
        "val_lbls    = torch.cat([torch.ones(len(e_val)),   torch.zeros(len(e_val_neg))],   dim=0).to(device)\n",
        "\n",
        "# — 8. Train with Early Stopping\n",
        "best_val_loss = float('inf')\n",
        "patience      = 10\n",
        "patience_cnt  = 0\n",
        "best_state    = None\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    Z = model.encode(X, A_norm)\n",
        "    logits = torch.stack([\n",
        "        (Z[idx_map[u]] * Z[idx_map[v]]).sum()\n",
        "        for u, v in train_pairs\n",
        "    ])\n",
        "    loss = criterion(logits, train_lbls)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Z_val      = model.encode(X, A_norm)\n",
        "        logits_val = torch.stack([\n",
        "            (Z_val[idx_map[u]] * Z_val[idx_map[v]]).sum()\n",
        "            for u, v in val_pairs\n",
        "        ])\n",
        "        val_loss = criterion(logits_val, val_lbls)\n",
        "\n",
        "    print(f\"Epoch {epoch:2d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    if val_loss.item() < best_val_loss:\n",
        "        best_val_loss = val_loss.item()\n",
        "        best_state    = model.state_dict()\n",
        "        patience_cnt  = 0\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        if patience_cnt >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "# — 9. Test evaluation on two splits\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Z_np = model.encode(X, A_norm).cpu().numpy()\n",
        "\n",
        "# -- Original test set\n",
        "orig_pairs = e_test + e_test_neg\n",
        "orig_lbls  = np.array([1]*len(e_test) + [0]*len(e_test_neg))\n",
        "orig_scores = [\n",
        "    float(\n",
        "        np.dot(Z_np[idx_map[u]], Z_np[idx_map[v]]) /\n",
        "        (np.linalg.norm(Z_np[idx_map[u]]) * np.linalg.norm(Z_np[idx_map[v]]) + 1e-9)\n",
        "    )\n",
        "    for u, v in orig_pairs\n",
        "]\n",
        "orig_auc = roc_auc_score(orig_lbls, orig_scores)\n",
        "orig_ap  = average_precision_score(orig_lbls, orig_scores)\n",
        "print(f\"\\nGCN + Cosine on Original test → AUC: {orig_auc:.4f}, AP: {orig_ap:.4f}\")\n",
        "\n",
        "# -- Rare test set (degree ≤ 25th percentile)\n",
        "threshold = np.percentile(list(deg.values()), 25)\n",
        "rare_pos  = [e for e in e_test     if deg[e[0]] <= threshold and deg[e[1]] <= threshold]\n",
        "rare_neg  = [e for e in e_test_neg if deg[e[0]] <= threshold and deg[e[1]] <= threshold]\n",
        "rare_pairs = rare_pos + rare_neg\n",
        "rare_lbls  = np.array([1]*len(rare_pos) + [0]*len(rare_neg))\n",
        "rare_scores = [\n",
        "    float(\n",
        "        np.dot(Z_np[idx_map[u]], Z_np[idx_map[v]]) /\n",
        "        (np.linalg.norm(Z_np[idx_map[u]]) * np.linalg.norm(Z_np[idx_map[v]]) + 1e-9)\n",
        "    )\n",
        "    for u, v in rare_pairs\n",
        "]\n",
        "rare_auc = roc_auc_score(rare_lbls, rare_scores)\n",
        "rare_ap  = average_precision_score(rare_lbls, rare_scores)\n",
        "print(f\"GCN + Cosine on Rare   test → AUC: {rare_auc:.4f}, AP: {rare_ap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92iUtdMGhWJ0",
        "outputId": "29599fb8-5fb4-4a5a-e058-7f40bd21b5de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch  1 | Train Loss: 0.6960 | Val Loss: 0.6858\n",
            "Epoch  2 | Train Loss: 0.6859 | Val Loss: 0.6712\n",
            "Epoch  3 | Train Loss: 0.6713 | Val Loss: 0.6504\n",
            "Epoch  4 | Train Loss: 0.6505 | Val Loss: 0.6250\n",
            "Epoch  5 | Train Loss: 0.6254 | Val Loss: 0.5995\n",
            "Epoch  6 | Train Loss: 0.6000 | Val Loss: 0.5792\n",
            "Epoch  7 | Train Loss: 0.5799 | Val Loss: 0.5672\n",
            "Epoch  8 | Train Loss: 0.5683 | Val Loss: 0.5638\n",
            "Epoch  9 | Train Loss: 0.5653 | Val Loss: 0.5645\n",
            "Epoch 10 | Train Loss: 0.5662 | Val Loss: 0.5627\n",
            "Epoch 11 | Train Loss: 0.5645 | Val Loss: 0.5559\n",
            "Epoch 12 | Train Loss: 0.5574 | Val Loss: 0.5456\n",
            "Epoch 13 | Train Loss: 0.5469 | Val Loss: 0.5350\n",
            "Epoch 14 | Train Loss: 0.5360 | Val Loss: 0.5266\n",
            "Epoch 15 | Train Loss: 0.5273 | Val Loss: 0.5213\n",
            "Epoch 16 | Train Loss: 0.5216 | Val Loss: 0.5182\n",
            "Epoch 17 | Train Loss: 0.5182 | Val Loss: 0.5154\n",
            "Epoch 18 | Train Loss: 0.5152 | Val Loss: 0.5120\n",
            "Epoch 19 | Train Loss: 0.5116 | Val Loss: 0.5079\n",
            "Epoch 20 | Train Loss: 0.5074 | Val Loss: 0.5036\n",
            "Epoch 21 | Train Loss: 0.5031 | Val Loss: 0.4998\n",
            "Epoch 22 | Train Loss: 0.4992 | Val Loss: 0.4972\n",
            "Epoch 23 | Train Loss: 0.4965 | Val Loss: 0.4959\n",
            "Epoch 24 | Train Loss: 0.4951 | Val Loss: 0.4955\n",
            "Epoch 25 | Train Loss: 0.4946 | Val Loss: 0.4953\n",
            "Epoch 26 | Train Loss: 0.4943 | Val Loss: 0.4947\n",
            "Epoch 27 | Train Loss: 0.4936 | Val Loss: 0.4937\n",
            "Epoch 28 | Train Loss: 0.4925 | Val Loss: 0.4929\n",
            "Epoch 29 | Train Loss: 0.4916 | Val Loss: 0.4924\n",
            "Epoch 30 | Train Loss: 0.4910 | Val Loss: 0.4923\n",
            "Epoch 31 | Train Loss: 0.4910 | Val Loss: 0.4926\n",
            "Epoch 32 | Train Loss: 0.4911 | Val Loss: 0.4927\n",
            "Epoch 33 | Train Loss: 0.4913 | Val Loss: 0.4926\n",
            "Epoch 34 | Train Loss: 0.4912 | Val Loss: 0.4922\n",
            "Epoch 35 | Train Loss: 0.4908 | Val Loss: 0.4917\n",
            "Epoch 36 | Train Loss: 0.4903 | Val Loss: 0.4912\n",
            "Epoch 37 | Train Loss: 0.4898 | Val Loss: 0.4908\n",
            "Epoch 38 | Train Loss: 0.4895 | Val Loss: 0.4907\n",
            "Epoch 39 | Train Loss: 0.4894 | Val Loss: 0.4907\n",
            "Epoch 40 | Train Loss: 0.4894 | Val Loss: 0.4906\n",
            "Epoch 41 | Train Loss: 0.4893 | Val Loss: 0.4903\n",
            "Epoch 42 | Train Loss: 0.4891 | Val Loss: 0.4900\n",
            "Epoch 43 | Train Loss: 0.4888 | Val Loss: 0.4897\n",
            "Epoch 44 | Train Loss: 0.4886 | Val Loss: 0.4896\n",
            "Epoch 45 | Train Loss: 0.4885 | Val Loss: 0.4896\n",
            "Epoch 46 | Train Loss: 0.4885 | Val Loss: 0.4895\n",
            "Epoch 47 | Train Loss: 0.4884 | Val Loss: 0.4894\n",
            "Epoch 48 | Train Loss: 0.4883 | Val Loss: 0.4891\n",
            "Epoch 49 | Train Loss: 0.4881 | Val Loss: 0.4889\n",
            "Epoch 50 | Train Loss: 0.4879 | Val Loss: 0.4888\n",
            "\n",
            "GCN + Cosine on Original test → AUC: 0.8419, AP: 0.8248\n",
            "GCN + Cosine on Rare   test → AUC: 0.8929, AP: 0.0576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCN + MLP"
      ],
      "metadata": {
        "id": "X-G0xqVcoosu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# — 1. Load edge list\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "df_edges = pd.read_csv(\"ingredient_cooccur_graph.csv\")\n",
        "edges     = list(zip(df_edges[\"source\"], df_edges[\"target\"]))\n",
        "weights   = {(u, v): w for u, v, w in zip(df_edges[\"source\"], df_edges[\"target\"], df_edges[\"weight\"]) }\n",
        "\n",
        "# — 2. Load features\n",
        "df_feat    = pd.read_csv(\"feature_matrix.csv\", index_col=\"node\")\n",
        "\n",
        "# —— 保持和训练图完全一致的节点顺序\n",
        "nodes_full = sorted(set(df_edges[\"source\"]) | set(df_edges[\"target\"]))\n",
        "df_feat    = df_feat.loc[nodes_full]\n",
        "\n",
        "# —— 构造张量\n",
        "X          = torch.tensor(df_feat.values.astype(np.float32), device=device)\n",
        "\n",
        "# —— 建立索引映射\n",
        "idx_map    = {n: i for i, n in enumerate(nodes_full)}\n",
        "\n",
        "\n",
        "# — 3. Split edges\n",
        "e_train, e_tmp  = train_test_split(edges, test_size=0.30, random_state=42)\n",
        "e_val,   e_test = train_test_split(e_tmp,   test_size=0.50, random_state=42)\n",
        "\n",
        "# — 4. Build train graph\n",
        "G_train = nx.Graph()\n",
        "G_train.add_nodes_from(nodes_full)\n",
        "G_train.add_weighted_edges_from([\n",
        "    (u, v, weights.get((u, v), weights.get((v, u), 1.0)))\n",
        "    for u, v in e_train\n",
        "])\n",
        "\n",
        "# compute degree for rare-split (you can use weighted degree if preferred)\n",
        "deg = dict(G_train.degree())\n",
        "\n",
        "# — 5. Negative sampling helper\n",
        "def sample_neg(n, forbidden, G):\n",
        "    negs = set()\n",
        "    while len(negs) < n:\n",
        "        u, v = np.random.choice(nodes_full, 2, replace=False)\n",
        "        if ((u, v) not in forbidden and (v, u) not in forbidden and not G.has_edge(u, v)):\n",
        "            negs.add((u, v))\n",
        "    return list(negs)\n",
        "\n",
        "forbidden   = set(edges)\n",
        "e_train_neg = sample_neg(len(e_train), forbidden, G_train)\n",
        "e_val_neg   = sample_neg(len(e_val),   forbidden, G_train)\n",
        "e_test_neg  = sample_neg(len(e_test),  forbidden, G_train)\n",
        "\n",
        "# — 6. Build normalized adjacency\n",
        "N = len(nodes_full)\n",
        "A = np.zeros((N, N), dtype=np.float32)\n",
        "for u, v in e_train:\n",
        "    i, j = idx_map[u], idx_map[v]\n",
        "    w    = weights.get((u, v), weights.get((v, u), 1.0))\n",
        "    A[i, j] = A[j, i] = w\n",
        "\n",
        "deg_array = A.sum(axis=1)\n",
        "D_inv_s   = np.diag(1.0 / np.sqrt(deg_array + 1e-9))\n",
        "A_norm    = torch.tensor(D_inv_s @ A @ D_inv_s, device=device)\n",
        "\n",
        "# — 7. Define GCN\n",
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim=64):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(in_dim, hid_dim)\n",
        "        self.lin2 = nn.Linear(hid_dim, hid_dim)\n",
        "    def encode(self, x, A):\n",
        "        h = A @ x\n",
        "        h = F.relu(self.lin1(h))\n",
        "        h = A @ h\n",
        "        h = F.relu(self.lin2(h))\n",
        "        return h\n",
        "\n",
        "hid_dim = 64\n",
        "decoder = nn.Sequential(\n",
        "    nn.Linear(hid_dim*2, hid_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hid_dim, 1)\n",
        ").to(device)\n",
        "\n",
        "# Instantiate\n",
        "model     = SimpleGCN(in_dim=X.shape[1], hid_dim=64).to(device)\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(decoder.parameters()),\n",
        "    lr=5e-3, weight_decay=1e-4\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Prepare data\n",
        "train_pairs = e_train + e_train_neg\n",
        "train_lbls  = torch.cat([torch.ones(len(e_train)), torch.zeros(len(e_train_neg))], dim=0).to(device)\n",
        "val_pairs   = e_val   + e_val_neg\n",
        "val_lbls    = torch.cat([torch.ones(len(e_val)),   torch.zeros(len(e_val_neg))],   dim=0).to(device)\n",
        "\n",
        "# — 8. Train with Early Stopping\n",
        "best_val_loss = float('inf')\n",
        "patience      = 10\n",
        "patience_cnt  = 0\n",
        "best_state    = None\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    decoder.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    Z = model.encode(X, A_norm)  # (N, hid_dim)\n",
        "\n",
        "    # 构造训练对的拼接向量\n",
        "    emb_pairs = torch.stack([\n",
        "        torch.cat([Z[idx_map[u]], Z[idx_map[v]]], dim=0)\n",
        "        for u, v in train_pairs\n",
        "    ])  # (num_pairs, hid_dim*2)\n",
        "\n",
        "    logits = decoder(emb_pairs).squeeze()        # (num_pairs,)\n",
        "    loss   = criterion(logits, train_lbls)       # BCEWithLogitsLoss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 验证同理\n",
        "    model.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        Z_val = model.encode(X, A_norm)\n",
        "        emb_val = torch.stack([\n",
        "            torch.cat([Z_val[idx_map[u]], Z_val[idx_map[v]]], dim=0)\n",
        "            for u, v in val_pairs\n",
        "        ])\n",
        "        logits_val = decoder(emb_val).squeeze()\n",
        "        val_loss   = criterion(logits_val, val_lbls)\n",
        "\n",
        "    print(f\"Epoch {epoch:2d} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss.item() < best_val_loss:\n",
        "        best_val_loss = val_loss.item()\n",
        "        best_state    = model.state_dict()\n",
        "        patience_cnt  = 0\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        if patience_cnt >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "# — 9. Test evaluation on two splits\n",
        "orig_pairs = e_test + e_test_neg\n",
        "orig_lbls  = np.array([1]*len(e_test) + [0]*len(e_test_neg))\n",
        "\n",
        "threshold = np.percentile(list(deg.values()), 25)\n",
        "rare_pos  = [e for e in e_test     if deg[e[0]] <= threshold and deg[e[1]] <= threshold]\n",
        "rare_neg  = [e for e in e_test_neg if deg[e[0]] <= threshold and deg[e[1]] <= threshold]\n",
        "rare_pairs = rare_pos + rare_neg\n",
        "rare_lbls  = np.array([1]*len(rare_pos) + [0]*len(rare_neg))\n",
        "\n",
        "def precision_at_k(labels: np.ndarray, scores: np.ndarray, k: int) -> float:\n",
        "    idx_desc = np.argsort(scores)[::-1][:k]\n",
        "    return labels[idx_desc].sum() / float(k)\n",
        "\n",
        "model.eval()\n",
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "    Z_final = model.encode(X, A_norm)\n",
        "\n",
        "    # 原始测试集\n",
        "    emb_test = torch.stack([\n",
        "        torch.cat([Z_final[idx_map[u]], Z_final[idx_map[v]]], dim=0)\n",
        "        for u, v in orig_pairs\n",
        "    ])\n",
        "    logits_test = decoder(emb_test).squeeze()\n",
        "    probs_test  = torch.sigmoid(logits_test).cpu().numpy()\n",
        "    orig_auc = roc_auc_score(orig_lbls, probs_test)\n",
        "    orig_ap  = average_precision_score(orig_lbls, probs_test)\n",
        "    print(f\"\\nGCN + MLP on Original test → AUC: {orig_auc:.4f}, AP: {orig_ap:.4f}\")\n",
        "\n",
        "    for K in [10, 20, 50]:\n",
        "        p_at_k = precision_at_k(orig_lbls, probs_test, K)\n",
        "        print(f\"Precision@{K} on Original test → {p_at_k:.4f}\")\n",
        "\n",
        "    # 稀有子集\n",
        "    emb_rare = torch.stack([\n",
        "        torch.cat([Z_final[idx_map[u]], Z_final[idx_map[v]]], dim=0)\n",
        "        for u, v in rare_pairs\n",
        "    ])\n",
        "    logits_rare = decoder(emb_rare).squeeze()\n",
        "    probs_rare  = torch.sigmoid(logits_rare).cpu().numpy()\n",
        "    rare_auc = roc_auc_score(rare_lbls, probs_rare)\n",
        "    rare_ap  = average_precision_score(rare_lbls, probs_rare)\n",
        "    print(f\"GCN + MLP on Rare   test → AUC: {rare_auc:.4f}, AP: {rare_ap:.4f}\")\n",
        "\n",
        "    for K in [10, 20, 50]:\n",
        "        p_at_k_rare = precision_at_k(rare_lbls, probs_rare, K)\n",
        "        print(f\"Precision@{K} on Rare test     → {p_at_k_rare:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SzXAas_koZa",
        "outputId": "6d6679b9-54c4-44b6-f212-ed16f193cc88"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch  1 | Train Loss: 0.6943 | Val Loss: 0.6915\n",
            "Epoch  2 | Train Loss: 0.6916 | Val Loss: 0.6886\n",
            "Epoch  3 | Train Loss: 0.6887 | Val Loss: 0.6841\n",
            "Epoch  4 | Train Loss: 0.6843 | Val Loss: 0.6773\n",
            "Epoch  5 | Train Loss: 0.6776 | Val Loss: 0.6673\n",
            "Epoch  6 | Train Loss: 0.6676 | Val Loss: 0.6531\n",
            "Epoch  7 | Train Loss: 0.6537 | Val Loss: 0.6342\n",
            "Epoch  8 | Train Loss: 0.6350 | Val Loss: 0.6105\n",
            "Epoch  9 | Train Loss: 0.6117 | Val Loss: 0.5826\n",
            "Epoch 10 | Train Loss: 0.5842 | Val Loss: 0.5522\n",
            "Epoch 11 | Train Loss: 0.5544 | Val Loss: 0.5220\n",
            "Epoch 12 | Train Loss: 0.5250 | Val Loss: 0.4950\n",
            "Epoch 13 | Train Loss: 0.4989 | Val Loss: 0.4734\n",
            "Epoch 14 | Train Loss: 0.4783 | Val Loss: 0.4568\n",
            "Epoch 15 | Train Loss: 0.4630 | Val Loss: 0.4452\n",
            "Epoch 16 | Train Loss: 0.4528 | Val Loss: 0.4381\n",
            "Epoch 17 | Train Loss: 0.4471 | Val Loss: 0.4296\n",
            "Epoch 18 | Train Loss: 0.4394 | Val Loss: 0.4167\n",
            "Epoch 19 | Train Loss: 0.4266 | Val Loss: 0.4006\n",
            "Epoch 20 | Train Loss: 0.4100 | Val Loss: 0.3834\n",
            "Epoch 21 | Train Loss: 0.3918 | Val Loss: 0.3666\n",
            "Epoch 22 | Train Loss: 0.3740 | Val Loss: 0.3513\n",
            "Epoch 23 | Train Loss: 0.3578 | Val Loss: 0.3381\n",
            "Epoch 24 | Train Loss: 0.3437 | Val Loss: 0.3270\n",
            "Epoch 25 | Train Loss: 0.3317 | Val Loss: 0.3174\n",
            "Epoch 26 | Train Loss: 0.3213 | Val Loss: 0.3085\n",
            "Epoch 27 | Train Loss: 0.3118 | Val Loss: 0.2996\n",
            "Epoch 28 | Train Loss: 0.3022 | Val Loss: 0.2900\n",
            "Epoch 29 | Train Loss: 0.2922 | Val Loss: 0.2803\n",
            "Epoch 30 | Train Loss: 0.2822 | Val Loss: 0.2713\n",
            "Epoch 31 | Train Loss: 0.2731 | Val Loss: 0.2638\n",
            "Epoch 32 | Train Loss: 0.2654 | Val Loss: 0.2576\n",
            "Epoch 33 | Train Loss: 0.2592 | Val Loss: 0.2520\n",
            "Epoch 34 | Train Loss: 0.2534 | Val Loss: 0.2469\n",
            "Epoch 35 | Train Loss: 0.2477 | Val Loss: 0.2422\n",
            "Epoch 36 | Train Loss: 0.2424 | Val Loss: 0.2387\n",
            "Epoch 37 | Train Loss: 0.2378 | Val Loss: 0.2357\n",
            "Epoch 38 | Train Loss: 0.2339 | Val Loss: 0.2324\n",
            "Epoch 39 | Train Loss: 0.2302 | Val Loss: 0.2301\n",
            "Epoch 40 | Train Loss: 0.2276 | Val Loss: 0.2273\n",
            "Epoch 41 | Train Loss: 0.2248 | Val Loss: 0.2245\n",
            "Epoch 42 | Train Loss: 0.2219 | Val Loss: 0.2235\n",
            "Epoch 43 | Train Loss: 0.2210 | Val Loss: 0.2216\n",
            "Epoch 44 | Train Loss: 0.2194 | Val Loss: 0.2207\n",
            "Epoch 45 | Train Loss: 0.2185 | Val Loss: 0.2202\n",
            "Epoch 46 | Train Loss: 0.2174 | Val Loss: 0.2213\n",
            "Epoch 47 | Train Loss: 0.2176 | Val Loss: 0.2194\n",
            "Epoch 48 | Train Loss: 0.2163 | Val Loss: 0.2179\n",
            "Epoch 49 | Train Loss: 0.2157 | Val Loss: 0.2169\n",
            "Epoch 50 | Train Loss: 0.2151 | Val Loss: 0.2164\n",
            "\n",
            "GCN + MLP on Original test → AUC: 0.9713, AP: 0.9705\n",
            "Precision@10 on Original test → 1.0000\n",
            "Precision@20 on Original test → 1.0000\n",
            "Precision@50 on Original test → 1.0000\n",
            "GCN + MLP on Rare   test → AUC: 0.8685, AP: 0.0289\n",
            "Precision@10 on Rare test     → 0.0000\n",
            "Precision@20 on Rare test     → 0.0000\n",
            "Precision@50 on Rare test     → 0.0200\n"
          ]
        }
      ]
    }
  ]
}